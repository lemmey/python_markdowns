___
[[inferential analysis]]
___
## Cause and effect

**Independent variables are the cause of the change in a Dependent variable.**

Changing the independent variable e.g. in a lab-experiment causes different outcomes in the dependent variable. The strength of that change is the effect, that a change in the independent variable causes. 
In early correlation testing, dependency might be veiled and does not need to be assumed.

## Type I (false positive) & Type II (false negative, the worst) error

As a result of chance a test result can falsely indicate an effect, or no effect, while the respective opposite is true. 

This chance of an undetected error is part of the reason why science never ‘proves‘ something but only indicates with a certain likelihood. Thus, we also  never find a true Null-hypothesis but instead it just wasn't rejected!

Note: The chance for Type I errors increases with the amount of tests taken!

Further,  considering 1.0 - POWER == Type II error rate, one can see that the statistical design is only as powerful as it‘s probability to create a false negative (II).
If you expect that the null hypothesis is probably true, a statistically significant result is probably a false positive. (E.g. it is unlikely that  there is an effect of this or that compound, but scarce evidence gives clues for about 1 in 1000 to have an effect, then 5% or 50 of 51 significant results are false positives and the ratio to true positives is frustrating. 
This is different when you expect an effect in halve the compounds. The  ratio from true to false positives lets you more confidently accept your results (also this dilemma is considered in Bayesian Statistics).